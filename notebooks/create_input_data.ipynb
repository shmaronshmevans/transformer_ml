{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from src.processing import hrrr_data\n",
    "from src.processing import nysm_data\n",
    "from src.processing import get_error\n",
    "from src.processing import normalize\n",
    "from src.processing import get_flag\n",
    "# from src.processing import create_data_for_vision\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_cats_path = \"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\"\n",
    "nysm_cats_df = pd.read_csv(nysm_cats_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Western Plateau', 'Eastern Plateau', 'Great Lakes',\n",
       "       'Hudson Valley', 'Coastal', 'Central Lakes', 'Mohawk Valley',\n",
       "       'Champlain Valley', 'Northern Plateau', 'St. Lawrence Valley'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nysm_cats_df['climate_division_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_drop(df):\n",
    "    df = df.drop(\n",
    "        columns=[\n",
    "            \"level_0\",\n",
    "            \"index_x\",\n",
    "            \"index_y\",\n",
    "            \"lead time\",\n",
    "            \"lsm\",\n",
    "            \"station_y\",\n",
    "            'lat',\n",
    "            'lon'\n",
    "        ]\n",
    "    )\n",
    "    df = df.rename(columns={'station_x':'station'})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_for_model(clim_div, today_date):\n",
    "    \"\"\"\n",
    "    This function creates and processes data for a vision transformer machine learning model.\n",
    "\n",
    "    Returns:\n",
    "        df_train (pandas DataFrame): A DataFrame for training the machine learning model.\n",
    "        df_test (pandas DataFrame): A DataFrame for testing the machine learning model.\n",
    "        features (list): A list of feature names.\n",
    "    \"\"\"\n",
    "    # load nysm data\n",
    "    nysm_df = nysm_data.load_nysm_data()\n",
    "    nysm_df.reset_index(inplace=True)\n",
    "    nysm_df = nysm_df.rename(columns={\"time_1H\": \"valid_time\"})\n",
    "\n",
    "    # load hrrr data\n",
    "    hrrr_df = hrrr_data.read_hrrr_data()\n",
    "\n",
    "    # Filter data by NY climate division\n",
    "    nysm_cats_path = \"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\"\n",
    "    nysm_cats_df = pd.read_csv(nysm_cats_path)\n",
    "    nysm_cats_df = nysm_cats_df[nysm_cats_df[\"climate_division_name\"] == clim_div]\n",
    "    stations = nysm_cats_df[\"stid\"].tolist()\n",
    "    nysm_df = nysm_df[nysm_df[\"station\"].isin(stations)]\n",
    "    hrrr_df = hrrr_df[hrrr_df[\"station\"].isin(stations)]\n",
    "\n",
    "    # need to create a master list for valid_times so that all the dataframes are the same shape\n",
    "    master_time = hrrr_df[\"valid_time\"].tolist()\n",
    "    for station in stations:\n",
    "        hrrr_dft = hrrr_df[hrrr_df[\"station\"] == station]\n",
    "        nysm_dft = nysm_df[nysm_df[\"station\"] == station]\n",
    "        times = hrrr_dft[\"valid_time\"].tolist()\n",
    "        times2 = nysm_dft[\"valid_time\"].tolist()\n",
    "        result = list(set(times) & set(master_time) & set(times2))\n",
    "        master_time = result\n",
    "    master_time_final = master_time\n",
    "\n",
    "    # Filter NYSM data to match valid times from master-list\n",
    "    nysm_df_filtered = nysm_df[nysm_df[\"valid_time\"].isin(master_time_final)]\n",
    "    hrrr_df_filtered = hrrr_df[hrrr_df[\"valid_time\"].isin(master_time_final)]\n",
    "\n",
    "    df_train_ls = []\n",
    "    df_test_ls = []\n",
    "    # merge dataframes so that each row is hrrr + nysm data for the same time step\n",
    "    # do this for each station individually\n",
    "    for station in stations:\n",
    "        print(f\"Compiling Data for {station}\")\n",
    "        nysm_df1 = nysm_df_filtered[nysm_df_filtered[\"station\"] == station]\n",
    "        hrrr_df1 = hrrr_df_filtered[hrrr_df_filtered[\"station\"] == station]\n",
    "\n",
    "        master_df = hrrr_df1.merge(nysm_df1, on=\"valid_time\")\n",
    "        master_df = columns_drop(master_df)\n",
    "\n",
    "        # Calculate the error using NWP data.\n",
    "        master_df = get_error.nwp_error(\"t2m\", master_df)\n",
    "        # encode for day_of_year\n",
    "        master_df = normalize.encode(master_df, \"day_of_year\", 366)\n",
    "\n",
    "        cols_to_carry = [\"valid_time\", \"station\", \"latitude\", \"longitude\"]\n",
    "        # master_df.to_parquet(\n",
    "        #     f\"/home/aevans/transformer_ml/src/data/temp_df/{today_date}/{clim_div}/{clim_div}_{station}.parquet\"\n",
    "        # )\n",
    "\n",
    "        new_df = master_df.drop(columns=cols_to_carry)\n",
    "\n",
    "        # for c in new_df.columns:\n",
    "        #     new_df[f'{c}_dt'] = new_df[c].diff()\n",
    "        # new_df = new_df.fillna(0)\n",
    "        new_df, features = normalize.normalize_df(new_df)\n",
    "        new_df = new_df.fillna(0)\n",
    "\n",
    "        # Split the data into training and testing sets.\n",
    "        length = len(new_df)\n",
    "        test_len = int(length * 0.8)\n",
    "        df_train = new_df.iloc[:test_len].copy()\n",
    "        df_test = new_df.iloc[test_len:].copy()\n",
    "        print(\"Test Set Fraction\", len(df_test) / len(new_df))\n",
    "\n",
    "        # Reintegrate the specified columns back into the training and testing DataFrames.\n",
    "        # for c in cols_to_carry:\n",
    "        #     df_train[c] = master_df[c]\n",
    "        #     df_test[c] = master_df[c]\n",
    "        df_train_ls.append(df_train)\n",
    "        df_test_ls.append(df_test)\n",
    "\n",
    "        print(\"train_shape\", df_train.shape)\n",
    "        print(\"test_shape\", df_test.shape)\n",
    "        gc.collect()\n",
    "        # print(\"train_start\", df_train['valid_time'].iloc[0])\n",
    "        # print(\"test_start\", df_test['valid_time'].iloc[0])\n",
    "    return df_train_ls, df_test_ls, features, stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Data for BURD\n",
      "Test Set Fraction 0.2000079242442252\n",
      "train_shape (40382, 35)\n",
      "test_shape (10096, 35)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_train, df_test, features, station \u001b[38;5;241m=\u001b[39m create_data_for_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCentral Lakes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 87\u001b[0m, in \u001b[0;36mcreate_data_for_model\u001b[0;34m(clim_div, today_date)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 87\u001b[0m     \u001b[43mgc\u001b[49m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# print(\"train_start\", df_train['valid_time'].iloc[0])\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;66;03m# print(\"test_start\", df_test['valid_time'].iloc[0])\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_train_ls, df_test_ls, features, stations\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gc' is not defined"
     ]
    }
   ],
   "source": [
    "df_train, df_test, features, station = create_data_for_model('Central Lakes', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_df = nysm_data.load_nysm_data()\n",
    "nysm_df.reset_index(inplace=True)\n",
    "nysm_df = nysm_df.rename(columns={\"time_1H\": \"valid_time\"})\n",
    "nysm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrrr_df = hrrr_data.read_hrrr_data()\n",
    "hrrr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path for tabular data.\n",
    "nysm_cats_path = \"/home/aevans/nwp_bias/src/landtype/data/nysm.csv\"\n",
    "nysm_cats_df = pd.read_csv(nysm_cats_path)\n",
    "nysm_cats_df = nysm_cats_df[nysm_cats_df['climate_division_name']=='Western Plateau']\n",
    "nysm_cats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = nysm_cats_df[\"stid\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_df = nysm_df[nysm_df['station'].isin(stations)]\n",
    "hrrr_df = hrrr_df[hrrr_df['station'].isin(stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nysm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrrr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to create a master list for valid_times so that all the dataframes are the same shape\n",
    "master_time = hrrr_df['valid_time'].tolist()\n",
    "for station in stations:\n",
    "    hrrr_dft = hrrr_df[hrrr_df[\"station\"] == station]\n",
    "    nysm_dft = nysm_df[nysm_df[\"station\"] == station]\n",
    "    times = hrrr_dft['valid_time'].tolist()\n",
    "    times2 = nysm_dft['valid_time'].tolist()\n",
    "    result = list(set(times) & set(master_time) & set(times2))\n",
    "    master_time = result\n",
    "\n",
    "master_time_final = master_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_me1 = (sorted(master_time_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_me1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(master_time_final))\n",
    "sorted(master_time_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(master_time))\n",
    "sorted(master_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for station in stations:\n",
    "    nysm_df1 = nysm_df[nysm_df['station']==station]\n",
    "    hrrr_df1 = hrrr_df[hrrr_df['station']==station]\n",
    "\n",
    "    master_df = hrrr_df1.merge(nysm_df1, on=\"valid_time\")\n",
    "    master_df = master_df.drop_duplicates(\n",
    "        subset=[\"valid_time\", \"t2m\"], keep=\"first\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = columns_drop(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the error using NWP data.\n",
    "master_df = get_error.nwp_error(\"t2m\", master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = normalize.encode(master_df, 'day_of_year', 366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = get_flag.get_flag(master_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_carry = ['valid_time', 'station', 'latitude', 'longitude', 'flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = master_df.drop(columns=cols_to_carry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df, features = normalize.normalize_df(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets.\n",
    "length = len(new_df)\n",
    "test_len = int(length * 0.8)\n",
    "df_train = new_df.iloc[:test_len].copy()\n",
    "df_test = new_df.iloc[test_len:].copy()\n",
    "print(\"Test Set Fraction\", len(df_test) / len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reintegrate the specified columns back into the training and testing DataFrames.\n",
    "for c in cols_to_carry:\n",
    "    df_train[c] = master_df[c]\n",
    "    df_test[c] = master_df[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
