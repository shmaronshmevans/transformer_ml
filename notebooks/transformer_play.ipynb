{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision.ops.misc import MLP, Conv2dNormActivation\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from functools import partial\n",
    "from typing import Any, Callable, Dict, List, NamedTuple, Optional\n",
    "\n",
    "from src.processing import create_data_for_vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data for Vision Transformer\n",
    "### We will target Western Plateau for consistency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Data for ADDI\n",
      "Test Set Fraction 0.2000117792567289\n",
      "train_shape (27166, 34)\n",
      "test_shape (6792, 34)\n",
      "Compiling Data for BELM\n",
      "Test Set Fraction 0.2000117792567289\n",
      "train_shape (27166, 34)\n",
      "test_shape (6792, 34)\n",
      "Compiling Data for COHO\n",
      "Test Set Fraction 0.2000117792567289\n",
      "train_shape (27166, 34)\n",
      "test_shape (6792, 34)\n",
      "Compiling Data for DELE\n",
      "Test Set Fraction 0.2000117792567289\n",
      "train_shape (27166, 34)\n",
      "test_shape (6792, 34)\n",
      "Compiling Data for ELMI\n",
      "Test Set Fraction 0.2000117792567289\n",
      "train_shape (27166, 34)\n",
      "test_shape (6792, 34)\n",
      "Compiling Data for GROV\n",
      "Test Set Fraction 0.2000117792567289\n",
      "train_shape (27166, 34)\n",
      "test_shape (6792, 34)\n",
      "Compiling Data for HART\n",
      "Test Set Fraction 0.2000117792567289\n",
      "train_shape (27166, 34)\n",
      "test_shape (6792, 34)\n",
      "Compiling Data for OLEA\n",
      "Test Set Fraction 0.2000117792567289\n",
      "train_shape (27166, 34)\n",
      "test_shape (6792, 34)\n",
      "Compiling Data for RAND\n",
      "Test Set Fraction 0.2000117792567289\n",
      "train_shape (27166, 34)\n",
      "test_shape (6792, 34)\n"
     ]
    }
   ],
   "source": [
    "df_train_ls, df_test_ls, features = create_data_for_vision.create_data_for_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['t2m', 'sh2', 'd2m', 'r2', 'u10', 'v10', 'tp', 'mslma', 'orog', 'tcc',\n",
       "       'asnow', 'cape', 'dswrf', 'dlwrf', 'gh', 'u_total', 'u_dir', 'new_tp',\n",
       "       'day_of_year_cos', 'day_of_year_sin', 'target_error', 'elev', 'tair',\n",
       "       'ta9m', 'td', 'relh', 'srad', 'pres', 'mslp', 'wspd_sonic',\n",
       "       'wmax_sonic', 'wdir_sonic', 'precip_total', 'snow_depth'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_ls[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStationDataset(Dataset):\n",
    "    def __init__(self, dataframes, target, features, past_steps, future_steps=1, nysm_vars=12):\n",
    "        \"\"\"\n",
    "        dataframes: list of station dataframes like in the SequenceDataset\n",
    "        target: same as SequenceDataset\n",
    "        features: same as SequenceDataset\n",
    "        sequence_length: SequenceDataset\n",
    "        \"\"\"\n",
    "        self.dataframes = dataframes\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "        self.past_steps = past_steps\n",
    "        self.future_steps = future_steps\n",
    "        self.nysm_vars = nysm_vars\n",
    "\n",
    "    def __len__(self):\n",
    "        shaper = min([self.dataframes[i].values.shape[0] - (self.past_steps + self.future_steps) for i in range(len(self.dataframes))])\n",
    "        return shaper\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # this is the preceeding sequence_length timesteps\n",
    "        x = torch.stack([torch.tensor(dataframe[self.features].values[i : i + self.past_steps + self.future_steps]) for dataframe in self.dataframes])\n",
    "        # stacking the sequences from each dataframe along a new axis, so the output is of shape (batch, stations (len(self.dataframes)), past_steps, features)\n",
    "        y = torch.stack([torch.tensor(dataframe[self.target].values[i + self.past_steps : i + self.past_steps + self.future_steps]) for dataframe in self.dataframes])\n",
    "        # this is (batch, stations, future_steps)\n",
    "        x[-self.future_steps:, :self.nysm_vars] = -999.0 # check that this is setting the right positions to this value\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiStationDataset(df_train_ls, 'target_error', features, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MultiStationDataset(df_test_ls, 'target_error', features, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(MLP):\n",
    "    \"\"\"Transformer MLP block.\"\"\"\n",
    "\n",
    "    _version = 2\n",
    "\n",
    "    def __init__(self, in_dim: int, mlp_dim: int, dropout: float):\n",
    "        super().__init__(in_dim, [mlp_dim, in_dim], activation_layer=nn.GELU, inplace=None, dropout=dropout)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.normal_(m.bias, std=1e-6)\n",
    "\n",
    "    def _load_from_state_dict(\n",
    "        self,\n",
    "        state_dict,\n",
    "        prefix,\n",
    "        local_metadata,\n",
    "        strict,\n",
    "        missing_keys,\n",
    "        unexpected_keys,\n",
    "        error_msgs,\n",
    "    ):\n",
    "        version = local_metadata.get(\"version\", None)\n",
    "\n",
    "        if version is None or version < 2:\n",
    "            # Replacing legacy MLPBlock with MLP. See https://github.com/pytorch/vision/pull/6053\n",
    "            for i in range(2):\n",
    "                for type in [\"weight\", \"bias\"]:\n",
    "                    old_key = f\"{prefix}linear_{i+1}.{type}\"\n",
    "                    new_key = f\"{prefix}{3*i}.{type}\"\n",
    "                    if old_key in state_dict:\n",
    "                        state_dict[new_key] = state_dict.pop(old_key)\n",
    "\n",
    "        super()._load_from_state_dict(\n",
    "            state_dict,\n",
    "            prefix,\n",
    "            local_metadata,\n",
    "            strict,\n",
    "            missing_keys,\n",
    "            unexpected_keys,\n",
    "            error_msgs,\n",
    "        )\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_1 = norm_layer(hidden_dim)\n",
    "        self.self_attention = nn.MultiheadAttention(hidden_dim, num_heads, dropout=attention_dropout, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        x = self.ln_1(input)\n",
    "        x, _ = self.self_attention(x, x, x, need_weights=False)\n",
    "        x = self.dropout(x)\n",
    "        x = x + input\n",
    "\n",
    "        y = self.ln_2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Transformer Model Encoder for sequence to sequence translation.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float,\n",
    "        attention_dropout: float,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Note that batch_size is on the first dim because\n",
    "        # we have batch_first=True in nn.MultiAttention() by default\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_dim).normal_(std=0.02))  # from BERT\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        for i in range(num_layers):\n",
    "            layers[f\"encoder_layer_{i}\"] = EncoderBlock(\n",
    "                num_heads,\n",
    "                hidden_dim,\n",
    "                mlp_dim,\n",
    "                dropout,\n",
    "                attention_dropout,\n",
    "                norm_layer,\n",
    "            )\n",
    "        self.layers = nn.Sequential(layers)\n",
    "        self.ln = norm_layer(hidden_dim)\n",
    "\n",
    "    def forward(self, input: torch.Tensor):\n",
    "        torch._assert(input.dim() == 3, f\"Expected (batch_size, seq_length, hidden_dim) got {input.shape}\")\n",
    "        input = input + self.pos_embedding\n",
    "        return self.ln(self.layers(self.dropout(input)))\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        stations: int,\n",
    "        past_timesteps: int,\n",
    "        future_timesteps: int,\n",
    "        patch_size: int = 16,\n",
    "        num_layers: int = 6,\n",
    "        num_heads: int = 8,\n",
    "        hidden_dim: int = 128,\n",
    "        mlp_dim: int = 768,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        num_classes: int = 1,\n",
    "        representation_size: Optional[int] = None,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.future_timesteps = future_timesteps\n",
    "        self.past_timesteps = past_timesteps\n",
    "        self.stations = stations\n",
    "        self.timesteps = future_timesteps + past_timesteps\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.representation_size = representation_size\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "        seq_length = stations * (future_timesteps + past_timesteps)\n",
    "\n",
    "        # Add a class token\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, 1, hidden_dim))\n",
    "        seq_length += 1\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            seq_length,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            hidden_dim,\n",
    "            mlp_dim,\n",
    "            dropout,\n",
    "            attention_dropout,\n",
    "            norm_layer,\n",
    "        )\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        if representation_size is None:\n",
    "            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
    "        else:\n",
    "            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
    "            heads_layers[\"act\"] = nn.Tanh()\n",
    "            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
    "\n",
    "        self.heads = nn.Sequential(heads_layers)\n",
    "\n",
    "        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n",
    "            fan_in = self.heads.pre_logits.in_features\n",
    "            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n",
    "            nn.init.zeros_(self.heads.pre_logits.bias)\n",
    "\n",
    "        if isinstance(self.heads.head, nn.Linear):\n",
    "            nn.init.zeros_(self.heads.head.weight)\n",
    "            nn.init.zeros_(self.heads.head.bias)\n",
    "\n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, h, w, c = x.shape\n",
    "        torch._assert(h == self.stations, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
    "        torch._assert(w == self.timesteps, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
    "\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, h * w, self.hidden_dim)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Classifier \"token\" is the future prediction - we will probably just want to select just some of these variables.\n",
    "        # x \\in (batch, stations * timesteps + 1, num_classes = 1)\n",
    "        x = x[:, -(self.stations * self.future_timesteps):, :] # this shape is (batch, stations, num_classes = 1)\n",
    "\n",
    "        x = self.heads(x) # is a linear transformation from hidden_dim to 1\n",
    "        \n",
    "        return x # logically we are saying return one value for the each future timestep for each station (interpreted as error)\n",
    "\n",
    "\n",
    "class AaronFormer(nn.Module):\n",
    "    def __init__(self, \n",
    "                output_dim, \n",
    "                stations,\n",
    "                past_timesteps=8,\n",
    "                future_timesteps=8,\n",
    "                patch_size=16,                 \n",
    "                num_layers=12,\n",
    "                num_heads=12,\n",
    "                hidden_dim=768,\n",
    "                mlp_dim=3072,\n",
    "                dropout=0.0, \n",
    "                attention_dropout=0.0,\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = VisionTransformer(\n",
    "                stations=stations,\n",
    "                past_timesteps=past_timesteps,\n",
    "                future_timesteps=future_timesteps,\n",
    "                patch_size=patch_size,\n",
    "                num_layers=num_layers,\n",
    "                num_heads=num_heads,\n",
    "                hidden_dim=hidden_dim,\n",
    "                mlp_dim=mlp_dim,\n",
    "                num_classes=output_dim,\n",
    "                dropout=dropout,\n",
    "                attention_dropout=attention_dropout\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AaronFormer(output_dim=1, \n",
    "stations = len(df_train_ls), \n",
    "past_timesteps=8,\n",
    "future_timesteps=8,\n",
    "patch_size=16,                 \n",
    "num_layers=12,\n",
    "num_heads=12,\n",
    "hidden_dim=768,\n",
    "mlp_dim=3072,\n",
    "dropout=0.0, \n",
    "attention_dropout=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 10\n",
    "LEARNING_RATE = 2e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Cross Entropy Loss\n",
    "loss_func = nn.MSELoss()\n",
    "# Use GPU if available  \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "if torch.cuda.is_available():\n",
    "    model.cuda() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer, device, epoch):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    ddp_loss = torch.zeros(2).to(device)\n",
    "\n",
    "    for batch_idx, (X, y) in enumerate(data_loader):\n",
    "        # Move data and labels to the appropriate device (GPU/CPU).\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass and loss computation.\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        # Zero the gradients, backward pass, and optimization step.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the total loss and the number of processed samples.\n",
    "        total_loss += loss.item()\n",
    "        ddp_loss[0] += loss.item()\n",
    "        ddp_loss[1] += len(X)\n",
    "\n",
    "    # Synchronize and aggregate losses in distributed training.\n",
    "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
    "\n",
    "    # Compute the average loss for the current epoch.\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    # Print the average loss on the master process (rank 0).\n",
    "    if rank == 0:\n",
    "        print(\"Train Epoch: {} \\tLoss: {:.6f}\".format(epoch, ddp_loss[0] / ddp_loss[1]))\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "\n",
    "def test_model(data_loader, model, loss_function, device):\n",
    "    # Test a deep learning model on a given dataset and compute the test loss.\n",
    "\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    # Set the model in evaluation mode (no gradient computation).\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize an array to store loss values.\n",
    "    ddp_loss = torch.zeros(3).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (X, y) in enumerate(data_loader):\n",
    "            # Move data and labels to the appropriate device (GPU/CPU).\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass to obtain model predictions.\n",
    "            output = model(X)\n",
    "\n",
    "            # Compute loss and add it to the total loss.\n",
    "            total_loss += loss_function(output, y).item()\n",
    "\n",
    "            # Update aggregated loss values.\n",
    "            ddp_loss[0] += F.mse_loss(output, y, reduction=\"sum\").item()\n",
    "            ddp_loss[2] += len(X)\n",
    "\n",
    "    # Calculate the average test loss.\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    # Synchronize and aggregate loss values in distributed testing.\n",
    "    dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
    "\n",
    "    # Print the test loss on the master process (rank 0).\n",
    "    if rank == 0:\n",
    "        test_loss = ddp_loss[0] / ddp_loss[2]\n",
    "        print(\n",
    "            \"Test set: Average loss: {:.4f}\\n\".format(test_loss)\n",
    "        )\n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'VisionTransformer' object has no attribute 'image_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ix_epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, ix_epoch)\n\u001b[0;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[1;32m      4\u001b[0m         train_loader, model, loss_func, optimizer, device, ix_epoch\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m test_model(test_loader, model, loss_func, device)\n\u001b[1;32m      7\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[155], line 12\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(data_loader, model, loss_function, optimizer, device, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Forward pass and loss computation.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, y)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Zero the gradients, backward pass, and optimization step.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[150], line 251\u001b[0m, in \u001b[0;36mAaronFormer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 251\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[150], line 202\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# Reshape and permute the input tensor\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m     n \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Expand the class token to the full batch\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[150], line 192\u001b[0m, in \u001b[0;36mVisionTransformer._process_input\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_process_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    191\u001b[0m     n, h, w, c \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 192\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_assert(h \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstations, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong image height! Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_assert(w \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrong image width! Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VisionTransformer' object has no attribute 'image_size'"
     ]
    }
   ],
   "source": [
    "for ix_epoch in range(1, EPOCHS + 1):\n",
    "    print(\"Epoch\", ix_epoch)\n",
    "    train_loss = train_model(\n",
    "        train_loader, model, loss_func, optimizer, device, ix_epoch\n",
    "    )\n",
    "    test_loss = test_model(test_loader, model, loss_func, device)\n",
    "    scheduler.step()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
